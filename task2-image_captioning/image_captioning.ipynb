{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a60a9bfa",
      "metadata": {
        "id": "a60a9bfa"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import json\n",
        "import string\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3933c334",
      "metadata": {
        "id": "3933c334"
      },
      "outputs": [],
      "source": [
        "from keras.applications.xception import Xception # importing cnn model for image processing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # importing tokenizer for vocabulary\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Input, add\n",
        "from keras.models import Model, load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "62673c4f",
      "metadata": {
        "id": "62673c4f"
      },
      "outputs": [],
      "source": [
        "def load_pickle_model(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        model=pickle.load(file)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1af278",
      "metadata": {
        "id": "2c1af278"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b1bc58a-ed83-4ea8-a3a0-ec280f963226",
      "metadata": {
        "id": "1b1bc58a-ed83-4ea8-a3a0-ec280f963226"
      },
      "outputs": [],
      "source": [
        "# load all captions in given file and cleaning them\n",
        "def load_captions_data(filename):\n",
        "    caption_dict={}\n",
        "    with open(filename, \"r\") as file:\n",
        "        data_lines=file.readlines()\n",
        "\n",
        "    for line in data_lines:\n",
        "        image_name=line.split('\\t')[0].split('#')\n",
        "\n",
        "        if not image_name[0] in caption_dict:\n",
        "            caption_dict[image_name[0]]=[]\n",
        "\n",
        "        cleaned_caption=clean_caption(line.split('\\t')[1][:-1])\n",
        "        caption_dict[image_name[0]].append(cleaned_caption)\n",
        "\n",
        "    return caption_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a5b2f10f-0029-4cc4-b9ce-c47e3bc2dc84",
      "metadata": {
        "id": "a5b2f10f-0029-4cc4-b9ce-c47e3bc2dc84"
      },
      "outputs": [],
      "source": [
        "# clean each caption\n",
        "def clean_caption(original_caption):\n",
        "    # tokenizing each word in lowercase and punctuations removed\n",
        "    tokens=original_caption.lower().translate({string.punctuation: ''}).split()\n",
        "\n",
        "    # remove all hanging 's and a\n",
        "    tokens=[token for token in tokens if len(token)>1]\n",
        "\n",
        "    # removing words with numbers in them\n",
        "    tokens=[token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # converting back to string\n",
        "    cleaned_caption='startseq ' + ' '.join(tokens) + ' endseq'\n",
        "\n",
        "    return cleaned_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d3289152-030f-4941-ab38-746497464c19",
      "metadata": {
        "id": "d3289152-030f-4941-ab38-746497464c19"
      },
      "outputs": [],
      "source": [
        "def save_data(data, filepath):\n",
        "    with open(filepath, 'w') as file:\n",
        "        json.dump(data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a39ccbe7-5f02-4f76-ab20-41225cb50907",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a39ccbe7-5f02-4f76-ab20-41225cb50907",
        "outputId": "55202de7-df76-40b4-98c5-943b54a02aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caption Data Loaded Successfully\n",
            "Caption Data Saved Successfully\n"
          ]
        }
      ],
      "source": [
        "filename=\"Flickr8k_Dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
        "\n",
        "caption_data=load_captions_data(filename)\n",
        "print(\"Caption Data Loaded Successfully\")\n",
        "\n",
        "save_data(caption_data, \"data_models/captions_file.json\")\n",
        "print(\"Caption Data Saved Successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140e4165",
      "metadata": {
        "id": "140e4165"
      },
      "source": [
        "Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "3476d4c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3476d4c3",
        "outputId": "ca50081d-6c13-451a-fdb4-94358aaef911"
      },
      "outputs": [],
      "source": [
        "model=Xception(include_top=False, weights=\"imagenet\", pooling='avg') # load cnn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "3f941f5f",
      "metadata": {
        "id": "3f941f5f"
      },
      "outputs": [],
      "source": [
        "# extract features of all images in the directory by using given cnn model\n",
        "def extract_features(model, directory):\n",
        "    image_files=os.listdir(directory)\n",
        "    features={}\n",
        "\n",
        "    for file in image_files:\n",
        "        filename=directory+'/'+file\n",
        "        image=Image.open(filename).resize((299,299))\n",
        "        image=np.expand_dims(image, axis=0)\n",
        "        image=image/255\n",
        "        feature=model.predict(image)\n",
        "        features[file]=feature\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d3713d",
      "metadata": {
        "id": "b1d3713d"
      },
      "outputs": [],
      "source": [
        "features=extract_features(model, \"Flickr8k_Dataset/Flicker8k_images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfba48cc",
      "metadata": {
        "id": "bfba48cc"
      },
      "outputs": [],
      "source": [
        "with open('data_models/extracted_image_features.pickle', 'wb') as file:\n",
        "    pickle.dump(features, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "4fc96dde",
      "metadata": {
        "id": "4fc96dde"
      },
      "outputs": [],
      "source": [
        "with open('data_models/xception_model.pickle', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2aad350",
      "metadata": {
        "id": "d2aad350"
      },
      "source": [
        "Loading Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "28ed95a5",
      "metadata": {
        "id": "28ed95a5"
      },
      "outputs": [],
      "source": [
        "# to get list of all image names in given file\n",
        "def load_image_names(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines=file.readlines()\n",
        "\n",
        "    # to remove '\\n' at the end of each line\n",
        "    names=list(map(lambda line: line[:-1], lines))\n",
        "    return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "563549d5",
      "metadata": {
        "id": "563549d5"
      },
      "outputs": [],
      "source": [
        "train_images=load_image_names(\"Flickr8k_Dataset/Flickr8k_text/Flickr_8k.trainImages.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1c980774",
      "metadata": {
        "id": "1c980774"
      },
      "outputs": [],
      "source": [
        "# to get description of each images in image_names\n",
        "def load_image_description(filename, image_names):\n",
        "    with open(filename, 'r') as file:\n",
        "        data=json.load(file)\n",
        "\n",
        "    description={}\n",
        "    for image in image_names:\n",
        "        description[image]=data[image]\n",
        "\n",
        "    return description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "13eed4ea",
      "metadata": {
        "id": "13eed4ea"
      },
      "outputs": [],
      "source": [
        "train_description=load_image_description(\"data_models/captions_file.json\", train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a967e194",
      "metadata": {
        "id": "a967e194"
      },
      "outputs": [],
      "source": [
        "# to get features of all images in image_names\n",
        "def load_image_features(filename, image_names):\n",
        "    with open(filename, 'rb') as file:\n",
        "        data=pickle.load(file)\n",
        "\n",
        "    features={}\n",
        "    for image in image_names:\n",
        "        features[image]=data[image]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c74f9ba1",
      "metadata": {
        "id": "c74f9ba1"
      },
      "outputs": [],
      "source": [
        "train_features=load_image_features('data_models/extracted_image_features.pickle', train_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3dc15d",
      "metadata": {
        "id": "1c3dc15d"
      },
      "source": [
        "Tokenizing Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4fbad798",
      "metadata": {
        "id": "4fbad798"
      },
      "outputs": [],
      "source": [
        "# converting dictionary of captions into list of tokenizing\n",
        "def dict_to_list(dict):\n",
        "    list=[]\n",
        "\n",
        "    for namesList in dict.values():\n",
        "        [list.append(name) for name in namesList]\n",
        "\n",
        "    return list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9f9d0ba4",
      "metadata": {
        "id": "9f9d0ba4"
      },
      "outputs": [],
      "source": [
        "# creating tokenizer fitting into text of all captions in the list\n",
        "# this will vectorize each text corpus (mapping some integer to a word in vocabulary)\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list=dict_to_list(descriptions)\n",
        "    tokenizer=Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "25bf89de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25bf89de",
        "outputId": "450f87a1-e2fa-4c52-f6b2-ec8d294aa537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7266"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer=create_tokenizer(train_description)\n",
        "with open(\"data_models/tokenizer.pickle\", \"wb\") as file:\n",
        "    pickle.dump(tokenizer, file)\n",
        "\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size # no. of words mapped in tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0a18ca46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a18ca46",
        "outputId": "5f454eac-e356-49a6-fe0d-ac1db55c386c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length=max(len(desc) for desc in dict_to_list(train_description))\n",
        "max_length # max length of any caption"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f640a1b",
      "metadata": {
        "id": "3f640a1b"
      },
      "source": [
        "Create a Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5ee49352",
      "metadata": {
        "id": "5ee49352"
      },
      "outputs": [],
      "source": [
        "# to create list of features, input sequences and output sequences for given given description list\n",
        "def create_sequence(feature, desc_list, tokenizer, max_length, vocab_size):\n",
        "    # features list, in_seq list, out_seq list\n",
        "    x1, x2, y=list(), list(), list()\n",
        "    for desc in desc_list:\n",
        "        # convert text to corresponding\n",
        "        sequence=tokenizer.texts_to_sequences([desc])[0]\n",
        "\n",
        "        # divide one sequence into in_seq and out_seq at each i\n",
        "        for i in range(1, len(sequence)):\n",
        "            in_seq, out_seq=sequence[:i], sequence[i]\n",
        "\n",
        "            in_seq=pad_sequences([in_seq], maxlen=max_length)[0] # to make each in_seq of size equal to max_length\n",
        "            out_seq=to_categorical([out_seq], num_classes=vocab_size)[0] # to encode out_seq to some class\n",
        "\n",
        "            x1.append(feature)\n",
        "            x2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "\n",
        "    return np.array(x1), np.array(x2), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "970ac93e",
      "metadata": {
        "id": "970ac93e"
      },
      "outputs": [],
      "source": [
        "# to generate data of all image features, their input sequences and output sequences\n",
        "def data_generator(descriptions, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    X1_batch, X2_batch, y_batch = [], [], []\n",
        "    for key, description_list in descriptions.items():\n",
        "        feature=features[key][0]\n",
        "        input_image, input_seq, output_seq=create_sequence(feature, description_list, tokenizer, max_length, vocab_size)\n",
        "        [X1_batch.append(image) for image in input_image]\n",
        "        [X2_batch.append(seq) for seq in input_seq]\n",
        "        [y_batch.append(output) for output in output_seq]\n",
        "\n",
        "        if len(X1_batch)>=batch_size:\n",
        "            yield (np.array(X1_batch[:batch_size]), np.array(X2_batch[:batch_size])), np.array(y_batch[:batch_size])\n",
        "            X1_batch, X2_batch, y_batch=X1_batch[batch_size:], X2_batch[batch_size:], y_batch[batch_size:]\n",
        "\n",
        "    if len(X1_batch)>0:\n",
        "        yield (np.array(X1_batch), np.array(X2_batch)), np.array(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a72f74e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72f74e2",
        "outputId": "bc31c8b9-f4d4-43ac-9835-4098440df606"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size=64\n",
        "generated_data=data_generator(train_description, train_features, tokenizer, max_length, vocab_size, batch_size)\n",
        "for x in generated_data:\n",
        "    batch_len=len(x[0][0])\n",
        "\n",
        "batch_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1cac2c8",
      "metadata": {
        "id": "c1cac2c8"
      },
      "source": [
        "Define CNN-RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "691db0be",
      "metadata": {
        "id": "691db0be"
      },
      "outputs": [],
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    # Features from CNN model compressed from 2048 into 256\n",
        "    inputs1=Input(shape=(2048,))\n",
        "    fl1=Dropout(0.5)(inputs1)\n",
        "    fl2=Dense(256, activation=\"relu\")(fl1)\n",
        "\n",
        "    # LSTM sequence model\n",
        "    inputs2=Input(shape=(max_length,))\n",
        "    sl1=Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    sl2=Dropout(0.5)(sl1)\n",
        "    sl3=LSTM(256)(sl2)\n",
        "\n",
        "    # Merge both models\n",
        "    decoder1=add([fl2, sl3])\n",
        "\n",
        "    decoder2=Dense(256, activation=\"relu\")(decoder1)\n",
        "    outputs=Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
        "\n",
        "    # Merge all layers into model\n",
        "    model=Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Summarize the model\n",
        "    print(model.summary())\n",
        "    # plot_model(model, to_file=\"data_models/model.png\", show_shapes=True)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b8b42a",
      "metadata": {
        "id": "f3b8b42a"
      },
      "source": [
        "Training our Image Caption Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "661b0860",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "661b0860",
        "outputId": "19677ed2-a6ff-410e-c6a0-9165a68f3d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size = 7266\n",
            "max length = 186\n",
            "no. of train features = 6000\n",
            "no. of train descriptions = 6000\n"
          ]
        }
      ],
      "source": [
        "print(\"vocab size =\", vocab_size)\n",
        "print(\"max length =\", max_length)\n",
        "print(\"no. of train features =\", len(train_features))\n",
        "print(\"no. of train descriptions =\", len(train_description))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "b6b6c10b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "b6b6c10b",
        "outputId": "1d637453-b8a4-4869-b7fd-5d81422a2475"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_10      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,860,096</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7266</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,867,362</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_10      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m1,860,096\u001b[0m │ input_layer_11[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_10[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_11[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_28 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7266\u001b[0m)      │  \u001b[38;5;34m1,867,362\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,843,106</span> (18.47 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,843,106\u001b[0m (18.47 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,843,106</span> (18.47 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,843,106\u001b[0m (18.47 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# defining our model\n",
        "model=define_model(vocab_size, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cff355e9",
      "metadata": {
        "id": "cff355e9"
      },
      "outputs": [],
      "source": [
        "# data generation\n",
        "batch_size=64\n",
        "generated_data=data_generator(train_description, train_features, tokenizer, max_length, vocab_size, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "Ht3510Lw5g4p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht3510Lw5g4p",
        "outputId": "3cf00281-c450-4b30-bedc-ee6eed8c4752"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "438cae76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "438cae76",
        "outputId": "af2121c7-c289-48b4-8d34-c37d40ae3b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 522ms/step - accuracy: 0.2773 - loss: 3.9245\n",
            "Epoch 2/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 576ms/step - accuracy: 0.2508 - loss: 4.2061\n",
            "Epoch 3/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 579ms/step - accuracy: 0.2537 - loss: 4.1466\n",
            "Epoch 4/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 622ms/step - accuracy: 0.2528 - loss: 4.0564\n",
            "Epoch 5/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 616ms/step - accuracy: 0.2714 - loss: 4.0100\n",
            "Epoch 6/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 640ms/step - accuracy: 0.2844 - loss: 3.8261\n",
            "Epoch 7/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 654ms/step - accuracy: 0.2457 - loss: 3.9192\n",
            "Epoch 8/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 628ms/step - accuracy: 0.2727 - loss: 3.8315\n",
            "Epoch 9/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 625ms/step - accuracy: 0.2653 - loss: 3.8423\n",
            "Epoch 10/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 680ms/step - accuracy: 0.2853 - loss: 3.7426\n",
            "Epoch 11/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 645ms/step - accuracy: 0.2634 - loss: 3.8438\n",
            "Epoch 12/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 630ms/step - accuracy: 0.2656 - loss: 3.7833\n",
            "Epoch 13/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 621ms/step - accuracy: 0.2644 - loss: 3.7580\n",
            "Epoch 14/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 628ms/step - accuracy: 0.2954 - loss: 3.5704\n",
            "Epoch 15/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 632ms/step - accuracy: 0.2716 - loss: 3.7700\n",
            "Epoch 16/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.2588 - loss: 3.8123\n",
            "Epoch 17/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 673ms/step - accuracy: 0.3067 - loss: 3.6625\n",
            "Epoch 18/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 643ms/step - accuracy: 0.3045 - loss: 3.6972\n",
            "Epoch 19/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 625ms/step - accuracy: 0.2715 - loss: 3.7948\n",
            "Epoch 20/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 624ms/step - accuracy: 0.2750 - loss: 3.8106\n",
            "Epoch 21/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 622ms/step - accuracy: 0.2978 - loss: 3.6368\n",
            "Epoch 22/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 620ms/step - accuracy: 0.2827 - loss: 3.6516\n",
            "Epoch 23/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 618ms/step - accuracy: 0.2571 - loss: 3.7684\n",
            "Epoch 24/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 641ms/step - accuracy: 0.3022 - loss: 3.6859\n",
            "Epoch 25/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 629ms/step - accuracy: 0.2913 - loss: 3.7056\n",
            "Epoch 26/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 621ms/step - accuracy: 0.3010 - loss: 3.6893\n",
            "Epoch 27/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 623ms/step - accuracy: 0.2803 - loss: 3.7029\n",
            "Epoch 28/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 623ms/step - accuracy: 0.3050 - loss: 3.5743\n",
            "Epoch 29/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 653ms/step - accuracy: 0.2824 - loss: 3.5862\n",
            "Epoch 30/30\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 629ms/step - accuracy: 0.2802 - loss: 3.8060\n"
          ]
        }
      ],
      "source": [
        "epochs=60\n",
        "batch_size=64\n",
        "# model training\n",
        "model.fit(generated_data, epochs=epochs, steps_per_epoch=ceil(len(train_description)/batch_size), verbose=1)\n",
        "\n",
        "model.save(\"data_models/image_captioning_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6d6ee839",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "6d6ee839",
        "outputId": "6620780e-82da-4f11-cbc0-a69c260d6556"
      },
      "outputs": [],
      "source": [
        "test_images=load_image_names(\"Flickr8k_Dataset/Flickr8k_text/Flickr_8k.testImages.txt\")\n",
        "test_description=load_image_description(\"data_models/captions_file.json\", test_images)\n",
        "test_features=load_image_features('data_models/extracted_image_features.pickle', test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "973839d1",
      "metadata": {
        "id": "973839d1"
      },
      "outputs": [],
      "source": [
        "generated_data=data_generator(test_description, test_features, tokenizer, max_length, vocab_size, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c7691f42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7691f42",
        "outputId": "ebe51527-69a4-4190-e3fd-d0c4dc9af4ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 147ms/step - accuracy: 0.2952 - loss: 3.8418\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[3.9012291431427, 0.2915624976158142]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(generated_data, steps=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192bf923",
      "metadata": {
        "id": "192bf923"
      },
      "source": [
        "Testing the Image Captioning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d8c255b3",
      "metadata": {
        "id": "d8c255b3"
      },
      "outputs": [],
      "source": [
        "# extract features of given image\n",
        "def extract_features(model, image):\n",
        "    image=image.resize((299,299))\n",
        "    image=np.expand_dims(image, axis=0)\n",
        "    image=image/255\n",
        "    feature=model.predict(image)\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b6ff9c11",
      "metadata": {
        "id": "b6ff9c11"
      },
      "outputs": [],
      "source": [
        "# generate description of given image\n",
        "def generate_description(model, tokenizer, features, max_length):\n",
        "    in_text=\"\"\n",
        "    sequence=np.zeros((1,max_length))\n",
        "    for i in range(max_length):\n",
        "        pred=model.predict([[features, sequence]], verbose=0)\n",
        "        token=np.argmax(pred)\n",
        "        word=tokenizer.index_word.get(token)\n",
        "        sequence[0][i]=token\n",
        "\n",
        "        if(not word):\n",
        "            break\n",
        "        in_text+=\" \"+word\n",
        "        if(word==\"endseq\"):\n",
        "            break\n",
        "\n",
        "    in_text=in_text.replace(\"startseq\", \"\")\n",
        "    in_text=in_text.replace(\"endseq\", \"\")\n",
        "\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "36b05318",
      "metadata": {
        "id": "36b05318"
      },
      "outputs": [],
      "source": [
        "xception_model=load_pickle_model(\"data_models/xception_model.pickle\")\n",
        "main_model=load_model(\"data_models/image_captioning_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "amjkmzp22SQp",
      "metadata": {
        "id": "amjkmzp22SQp"
      },
      "outputs": [],
      "source": [
        "tokenizer=load_pickle_model(\"data_models/tokenizer.pickle\")\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "max_length=186"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e9680e81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9680e81",
        "outputId": "9a9595ee-7420-4a36-9ec4-680910954890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting Image Features...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955ms/step\n",
            "Generating Image Caption\n",
            "Image Caption:  the orange dog is running through the air \n"
          ]
        }
      ],
      "source": [
        "image_path=\"Flickr8k_Dataset/Flicker8k_images/44856031_0d82c2c7d1.jpg\"\n",
        "image=None\n",
        "try:\n",
        "    image=Image.open(image_path)\n",
        "except:\n",
        "    print(\"Invalid Image!\")\n",
        "\n",
        "if(image):\n",
        "    print(\"Extracting Image Features...\")\n",
        "    features=extract_features(xception_model, image)\n",
        "    print(\"Generating Image Caption\")\n",
        "    generated_desc=generate_description(main_model, tokenizer, features, max_length)\n",
        "    print(\"Image Caption:\", generated_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cac2a46",
      "metadata": {
        "id": "6cac2a46"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
